---
title: 番外篇：高可用
taxonomy:
    category: docs
---

笔者相信，很多人都像我一样做过思想实验，希望设计一个“完全高可用”的系统，但是最终可能都败下了阵来，为什么？因为高可用和其它常见的分布式系统需求是互斥的。

数据重要如银行，也只是要求在天灾面前要尽量不丢数据、少丢数据，凭什么你就要求自己的系统永远可用呢？其实，想从架构层面实现高可用是非常困难的，终极高可用就是将数据完整地复制到世界各地的所有节点上，并用超长的时间来达到完全一致，这是什么，这是区块链呀。

高可用和性能、一致性都是冲突的，只能采用策略尽量压制问题。

### 熔断

这个词在技术圈的流行应该有微博一半功劳，压力一大就熔断：主动停止不重要的服务，断尾自救，争取让核心业务不挂。

### 限流

限制一部分地区、一部分用户的访问，以保护整个集群不崩，一般用于限制单个用户对系统造成的压力过大，对面很可能是机器人。

### 笔者对于设备故障的经验

在机房硬件设备中，最容易损坏的肯定是磁盘（硬盘），因为它是在不断磨损的，即便是 SSD 也会随着时间的流逝寿命逐渐丢失，哪怕装在盒子里不通电也一样。但是硬盘之后，容易宕机或者损坏的是什么大家应该就猜不到了。

首先是断电：数据中心因为各种问题断电是最常见的故障，这个故障的概率甚至要高于软件引发的故障和电源适配器的故障。

其次是内存失效：内存以及内存插槽，内存电路，似乎是今天服务器硬件之中除了磁盘之外最容易坏的东西，我们刚刚已经进入了 DDR5 时代，内存功耗又上了一个台阶，恐怕故障率会进一步上升。

然后是网络线材和供电线材接口的问题，时间长了松了或者进灰了，就会丢包或者重启。2011 年震惊世界的中微子超光速事件，就是插头松了导致的。

最后是高温引发的宕机：特别是 GPU 服务器，一旦服务器或者机房的散热系统出现问题，服务器很容易就主动限制性能甚至关机。

其它的，硬件网络设备（路由器交换机）故障、彻底挖断光纤、CPU 损坏、电源转换器寿命耗尽、主板电池故障、地震火灾洪水等，可能在一台服务器的上架寿命之中都完全无法遇到。

### 机房进水导致服务器损坏事件

就在笔者完成第一版书稿的几天后，住范儿公司网络机房内部署的一些非关键业务的服务器遭遇了进水损坏这个致命的问题。这个故事告诉我们，很多看起来很小概率的事故，是真的有可能荒谬地发生的，数据异地备份十分重要。

### Facebook 2021年 10 月 4 号宕机

Facebook 的用户不可谓不多，对高可用的投入不可谓不足，为什么还是会 [整个公司完全宕机 7 小时](https://zh.wikipedia.org/wiki/2021%E5%B9%B4Facebook%E7%95%B6%E6%A9%9F%E4%BA%8B%E4%BB%B6) 呢？

事故的起因是一个错误的命令意外断开了 Facebook 的 DNS 服务，结果问题大了：

1. 所有客户端 API 失效，用户无法获得任何信息
2. 数据中心 VPN 服务失效，无法远程登录到数据中心内的设备上
3. 亲自去机房，发现门禁卡刷不开门，在暴力破拆后才接触到物理设备，插上显示器和键盘才能解决问题
4. 邮件、Google 文档、Zoom 都登不上
5. 办公大楼的门禁卡系统也失效了，无法刷开会议室的门，甚至无法离开办公楼

结合阿里云香港一个数据中心 [因为空调故障导致整个数据中心宕机超过 24 小时](https://help.aliyun.com/noticelist/articleid/1061819219.html)，认命吧，商业机构做不了真正的高可用的：资源使用率就是钱呐。
