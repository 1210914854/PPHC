---
menu: 突破单台服务器的性能极限
title: 突破单台服务器的性能极限：从 20G 到 200G
taxonomy:
    category: docs
---

借助 Keepalived，我们成功搭建了高可用的 LVS 集群。现在，让我们来攻克下一个难题：将单台服务器的性能从 20G 提升至 200G。

### LVS 单机性能为何会卡在 20G

最新的 AMD EPYC™ 9654 服务器 CPU 拥有 192 个物理核心，双路平台共有 768 个 vCore，然而 LVS 单机性能却停滞在 20G 附近。为何无法进一步提升性能呢？原因在于 Linux 网络栈已经达到了性能极限。

### Linux 网络栈优化

由于 LVS 基于 Linux 内核中的 Netfilter，依赖于 Linux 网络栈，数据包的发送和接收本身就需要读写内存，而用户态和内核态的转换需要上下文切换，还需要读写内存。在高带宽需求下，相对耗时的内存读写成为了阻止性能进一步提升的主要障碍。DPDK 和 NPU 硬件卸载是解决这一问题的两个方案。

#### DPDK

DPDK（Intel 开源的高性能网络数据处理框架）通过申请大页内存和轮询代替中断这两个关键特性，为高速率网卡提供了一种高性能解决方案。其 CPU 亲和性、多核调度架构以及内存 NUMA 优化等基础架构进一步推高了性能，使其成为用户态网络界面框架的首选。

爱奇艺开源的 [DPVS](https://github.com/iqiyi/dpvs) 就是 DPDK 技术在负载均衡领域的成功应用。在 10G 网络下进行小包测试，DPVS 的性能是标准 LVS 的五倍。在这里我们保守一点，将其折半为 2.5 倍，那么 DPVS 的单网卡性能极限就是 50G。

#### 网卡芯片硬件卸载

最新的 NPU（网络处理器单元）已经支持了许多硬件卸载特性，包括 IP 分片、TCP 分段、小包重组、checksum 校验、硬件 QOS，以及最重要的 VxLAN（虚拟扩展本地局域网）的剥离和插入。这些功能是 DPVS 的重要组成部分，可以减少数据流之间的干扰，大幅提升系统总容量。

为什么说 VxLAN 最重要？因为它就是当前云服务商给每个租户搭建 VPC 所使用的最主流的技术。

此外，RDMA（远程直接内存访问）技术也是网卡芯片的一个重要发展方向，我们将在后面详细探讨数据库计算和存储分离时再深入介绍。

### 全局锁优化

除了 Linux 网络栈的限制，LVS 本身架构上的全局锁也是一个突破口。全局锁导致了海量的 CPU 核心无法被利用。我们可以借鉴阿里云的处理思路。

#### 数据包亲和性优化

阿里云通过 RSS 技术将同一个五源组报文扔到同一个 CPU 上处理，确保入方向的所有相同连接上的报文都能交给相同的 CPU 处理。同时，在每个核转发出去时都使用当前 CPU 上的本地地址，并通过设置一些 fdir 规则，使报文回来时后端服务器访问的目的地址与对应 CPU 上的本地地址匹配。这样就能实现同一连接上左右方向的报文都被同一个 CPU 处理，将存储“五元组对应关系”的内存数据库在不同的 CPU 核心上隔离开，从而实现整体系统容量的线性提升。参考资料：[高性能负载均衡设计与实现](https://zhuanlan.zhihu.com/p/29949340)。

### 性能问题需靠架构解决

当单网卡性能达到极限 50G 时，如何将集群性能提升至 200G 呢？插入四块网卡可行吗？理论上可行，但现实中如此大流量的系统必须借助架构来提升容量，因为流量不可能凭空一分为四，此外还需要解决高可用问题。

![](https://qn.lvwenhan.com/2023-01-09-16732656558819.jpg)

<center>图 6-6 200G 负载均衡集群架构图</center>

如图 6-6 所示就是笔者设计的 200G 负载均衡集群的架构图，下面我们从左到右解析该架构。

#### 1. 拆除 IP 单点：朴素的 DNS

基于 DNS 技术的单域名多 IP 扩容技术曾是第一代负载均衡技术：朴素的 DNS 协议可将用户终端流量直接导向全国多个机房，实现真正的性能倍增，尤其适合 Web 1.0 时代的静态网页和搜索引擎等无数据同步需求的业务。

在当年电信、网通、铁通、教育网之间只有 40G 小水管的年代，DNS 技术极大地提升了普通用户获取网络资源的速度，提升了用户体验：引导每个运营商的宽带用户访问放在该运营商机房的服务器，可实现原始的“手搓 CDN”架构。最后一章我们还会谈到 DNS 技术在终极高可用架构中的价值。

经过 DNS 拆分，每个路由拥有一个公网 IP，承载 100G 带宽。需要注意的是，此处的路由真的是路由，并非网关，它只需告诉每个数据包的下一跳（该去何处）是哪个 IP 即可。硬件路由设备的性能非常强大，承受住 200G 带宽甚至不需要高端数据中心核心设备。此架构中的网关由后面的 LVS 充当。

#### 2. 拆除交换机单点：OSPF/ECMP 路由技术

上图中的 `路由_1` 和 `路由_2` 采用支持 ECMP 的硬件设备或软件路由来充当，它们会将流量平均分配给两个交换机。每台交换机承载 100G 带宽，对交换机来说简直是小菜一碟，2 万人民币的硬件交换机就能实现 24 口 x 100G 的全线速交换。

#### 3. 拆除服务器单点：LVS 单机双网卡四网口

每台 LVS 服务器都安装双口 100G 网卡 2 张，共有四个网口，这样单机可以实现 50G x 2 的极限性能。每个网卡上的两个网口分别连接两台交换机，既实现了高可用性，又保证了协议速度（线速）不会成为瓶颈。

#### 4. 天下无单点：全冗余架构

大家可以看到，从左至右，整个架构的每一层的每一个节点都与左边一层的所有节点进行连接，这种全冗余架构可满足任意一台设备宕机时整个系统仍可用，甚至系统容量都能保持不变。

如果某台路由宕机，则另一台路由会将其公网 IP 接过来，实现分钟级故障恢复（公网路由表更新较为缓慢）。

如果某台交换机宕机，则左侧路由通过 ECMP 及时调整路由配置，实现秒级切换。

如果某台 LVS 服务器宕机：Keepalived 机制会让 Standby 设备在 1 秒内顶上。

### 系统容量计算

对于整个系统而言，四台安装了双网卡的 LVS 服务器（2 主 2 备）在两个公网 IP 入口的情况下，可以实现总带宽为 50G x 2 x 2 = 200G。即使其中任意一台设备宕机，也不会影响系统的总容量。

然而，200G 并非该系统的极限。如果我们让一组两台 LVS 服务器使用两个 VIP 进行互为主备的双活配置，可以将整个系统的容量提升到理论极限的 400G。接下来，我们将分析各种单项资源的性能极限：

1. 单个公网 IP：最大带宽为系统最大容量的 200G。
2. 单个 VIP：最大带宽为 100G。
3. 单个数据流：最大带宽为单网卡性能极限的 50G。

还记得之前提到的价值 100 万的硬件负载均衡器吗？它的最大 L4 带宽仅为 40G。而我们这套 200/400G 的设备需要多少钱呢？

1x2 + 2x2 + 3x4 = 18万

#### 额外的优势

花费 18 万搭建一套图 6-6 中的设备，不仅可以构建一套 200G 的集群，其中的路由和交换机还拥有大量的容量可用于其他业务。

更重要的是：LVS 集群相对于硬件负载均衡设备来说，可以轻松控制且可编程。这对于右侧上游服务器的网络架构规划非常有利。对于云计算厂商而言，可以开发更丰富的功能来提供更复杂的服务（例如计算和存储分离的数据库），提高硬件资源的利用率，同时可以为云平台用户提供更复杂的功能：

> 软件带来的无限可能才是这套配置最核心的价值！
